<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>数据库存储</title>
    <link href="/2022/12/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8/"/>
    <url>/2022/12/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="数据库存储"><a href="#数据库存储" class="headerlink" title="数据库存储"></a>数据库存储</h1><h2 id="计算机存储体系"><a href="#计算机存储体系" class="headerlink" title="计算机存储体系"></a>计算机存储体系</h2><p>如图，在DRAM(内存)及以上为易失性(volatile)存储,即数据容易丢失（需要通电维持）。在此之下为非易失性存储，即数据不容易丢失，可以在不通电情况下保存。</p><p><img src="/2022/12/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8/Snipaste_2022-12-18_05-10-22.png" alt></p><p>而易失性存储可以字节可寻址，即支持随机访问。而对非易失性存储，它们具有的是块寻找能力。此外，它们的访问速度也是由上到下。</p><p><img src="/2022/12/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8/Snipaste_2022-12-18_05-21-15.png" alt></p><p>Q:数据库的关键是将数据从磁盘送人内存，那为什么不全部交由操作系统管理呢</p><p>A:当我们只读访问数据时，这交给操作系统管理是方便的，但是并发写数据时情况会复杂起来</p>]]></content>
    
    
    
    <tags>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/12/15/%E8%AE%BA%E6%96%87/"/>
    <url>/2022/12/15/%E8%AE%BA%E6%96%87/</url>
    
    <content type="html"><![CDATA[<h1 id="操作技能量化"><a href="#操作技能量化" class="headerlink" title="操作技能量化"></a>操作技能量化</h1><ol><li><p>第一部分为飞行员 驾驶飞机应具备的基本技能：①完成既定的任务，如：是否将飞机停在正确的停机位； ②在完成任务的过程中必须符合相关规定，不能违反飞行规则。</p></li><li><p>第二部分为飞行员操 作的熟练程度：①操作是否及时，如：执行拉平时的速度、高度是否准确；②操作是否稳定，如是否平滑地操纵驾驶杆。</p></li></ol><p>   其中操作的平稳性、规范性是用来评价飞行员对控制部件操作的优劣，完成度和及时性是根据飞机表现出来的特征评价飞行员的操作技能。</p><p><img src="/2022/12/15/%E8%AE%BA%E6%96%87/Users\van\AppData\Roaming\Typora\typora-user-images\image-20221215062200962.png" alt></p><h1 id="功率谱分析的方法"><a href="#功率谱分析的方法" class="headerlink" title="功率谱分析的方法"></a>功率谱分析的方法</h1><h1 id="熵权TOPSIS模型"><a href="#熵权TOPSIS模型" class="headerlink" title="熵权TOPSIS模型"></a>熵权TOPSIS模型</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达-机器学习-多分类问题</title>
    <link href="/2022/12/14/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    <url>/2022/12/14/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h1><p><img src="/2022/12/14/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/Snipaste_2022-12-14_19-01-23.png" alt></p><p>目标取值在一个离散集合中，而不是像以前的分类问题一样取值只要两个。</p><p>其对应的公式为</p><script type="math/tex; mode=display">z_j=\vec{w_j} \cdot \vec{x} +b_j j=1,...,N \\a_j= \frac{e^{z_j}}{\sum_{k=1}^{N}e^{z_k}}=P(y=j|\vec{x} )</script><p>下面是一个计算例子</p><p><img src="/2022/12/14/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/Snipaste_2022-12-14_19-20-56.png" alt></p><p>损失函数定义为</p><script type="math/tex; mode=display">loss(a_1,...,a_N,y)=\begin {cases} {-loga_1 \quad if \quad y=1}\\                      -loga_2 \quad if \quad y=2 \\                      ...\\                      -loga_N \quad if \quad y=N                      \end{cases}</script><h1 id="神经网络中的softmax"><a href="#神经网络中的softmax" class="headerlink" title="神经网络中的softmax"></a>神经网络中的softmax</h1><p><img src="/2022/12/14/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/Snipaste_2022-12-14_19-38-45.png" alt></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs231n-图像分类-线性分类</title>
    <link href="/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/"/>
    <url>/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<script type="math/tex; mode=display">f(x,W)=Wx+b</script><p><img src="/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/Snipaste_2022-12-06_16-06-58.png" alt></p><p><strong>example:</strong></p><p><img src="/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/Snipaste_2022-12-06_16-08-23.png" alt></p>]]></content>
    
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs231n-图像分类-数据驱动方法</title>
    <link href="/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>数据驱动的方法:</p><ol><li>收集图像和标签的数据集(Collect a dataset of images and labels)</li><li>使用机器学习来训练分类器(Use Machine learning to train a classifier)</li><li>用分类器来预测新的图片(Evaluate the classifier on new images)</li></ol><h1 id="Nearest-Neighbor-classifier"><a href="#Nearest-Neighbor-classifier" class="headerlink" title="Nearest Neighbor classifier"></a>Nearest Neighbor classifier</h1><p>比较图像的方法:</p><p><strong>L1 distance</strong>:                 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.46ex;" xmlns="http://www.w3.org/2000/svg" width="23.21ex" height="4.277ex" role="img" focusable="false" viewbox="0 -803.3 10258.7 1890.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mn" transform="translate(553,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(956.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1345.6,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mn" transform="translate(473,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(2222.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(2666.8,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mn" transform="translate(473,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(3543.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(4210.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="munder" transform="translate(5265.9,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="mi" transform="translate(350.2,-850) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g><g data-mml-node="mo" transform="translate(6488.5,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msubsup" transform="translate(6766.5,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(590.2,490.8) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mn" transform="translate(473,-297.3) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(7984.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="msubsup" transform="translate(8984.9,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(590.2,490.8) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mn" transform="translate(473,-297.3) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(9980.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g></g></g></svg></mjx-container></p><p><strong>L2 distance:</strong>                <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.883ex;" xmlns="http://www.w3.org/2000/svg" width="26.63ex" height="5.566ex" role="img" focusable="false" viewbox="0 -1185.6 11770.6 2460"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mn" transform="translate(553,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(956.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1345.6,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mn" transform="translate(473,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(2222.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msub" transform="translate(2666.8,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mn" transform="translate(473,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(3543.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(4210.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msqrt" transform="translate(5265.9,0)"><g transform="translate(1020,0)"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="mi" transform="translate(350.2,-850) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g><g data-mml-node="mo" transform="translate(1056,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(1445,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(590.2,490.8) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mn" transform="translate(473,-297.3) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(2663.1,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="msubsup" transform="translate(3663.3,0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"/></g><g data-mml-node="mi" transform="translate(590.2,490.8) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mn" transform="translate(473,-297.3) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="msup" transform="translate(4659.2,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mn" transform="translate(422,289) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g><g data-mml-node="mo" transform="translate(0,-324.4)"><path data-c="221A" d="M424 -948Q422 -947 313 -434T202 80L170 31Q165 24 157 10Q137 -21 137 -21Q131 -16 124 -8L111 5L264 248L473 -720Q473 -717 727 359T983 1440Q989 1450 1001 1450Q1007 1450 1013 1445T1020 1433Q1020 1425 742 244T460 -941Q458 -950 439 -950H436Q424 -950 424 -948Z"/></g><rect width="5484.7" height="60" x="1020" y="1065.6"/></g></g></g></svg></mjx-container></p><p><img src="/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%96%B9%E6%B3%95/Snipaste_2022-12-06_14-10-50.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NearestNeighbor</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self,X,y</span>):<br>        self.Xtr=X <span class="hljs-comment">#N*D each row is a example</span><br>        self.ytr=y<span class="hljs-comment"># y is 1*N label</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self,X</span>):<br>        num_test=X.shape[<span class="hljs-number">0</span>]<br>        y_pred=np.zeros(num_test,dtype=self.ytr.dtype)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            distance=np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(self.Xtr-self.X[i,:]),axis=<span class="hljs-number">1</span>)<span class="hljs-comment">#axis=1 mean get a N*1 array</span><br>            idx=np.argmin(distance)<span class="hljs-comment"># get the index with smallest distance</span><br>            y_pred[i]=self.y[idx]<br><br>        <span class="hljs-keyword">return</span> y_pred<br><br></code></pre></td></tr></table></figure><p>Q:给定N个样本，训练和预测的时间有多快</p><p>A:Train O(1),Predict O(N).因此训练只是保存了数据。这并不好，我们希望分类器在预测时是快速的，而训练时很慢是可以接受的</p><h1 id="K-Nearest-Neighbors"><a href="#K-Nearest-Neighbors" class="headerlink" title="K-Nearest Neighbors"></a>K-Nearest Neighbors</h1><p>一个样本与数据集中的K个样本最相似，如果这K个样本中的大多数属于某一类别，则该样本也属于改类别。其中的K属于超参数(Hyperparameters)，意味着这个参数更多是我们设定的而不是直接从数据中学习到的。</p><p><img src="/2022/12/06/cs231n-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%96%B9%E6%B3%95/Snipaste_2022-12-06_14-41-08.png" alt></p>]]></content>
    
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达-机器学习-分类</title>
    <link href="/2022/11/30/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB/"/>
    <url>/2022/11/30/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>逻辑回归并不属于回归预测模型，其属于分类</p><p>为了输出逻辑回归预测的分类结果，我们希望输出结果在0-1之间,</p><script type="math/tex; mode=display">g(\mathrm z)= \frac{1}{1+e^{-z}}(0<g(\mathrm z)<1)</script><p>这是常用的sigmoid function,z就是输入，而输出可以用于表示为正例的概率</p><p><img src="/2022/11/30/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB/Snipaste_2022-11-30_20-12-48.png" alt></p><p>也就是说</p><script type="math/tex; mode=display">f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}})= \frac {1}{1+e^{-{\overrightarrow{w}}.{\overrightarrow {x}}+b}}</script><p>而决策边界就是一个能正确区分正例和负例之间的边界</p><p><img src="/2022/11/30/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB/Snipaste_2022-11-30_20-32-49.png" alt></p><p>而逻辑回归的损失函数定义为</p><script type="math/tex; mode=display">L\left(f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)}), y^{(i)}\right)=\left\{\begin{aligned}-\log \left(f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})\right) & \quad \text { if } y^{(i)}=1 \\-\log \left(1-f_{ \overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})\right) & \quad \text { if } y^{(i)}=0 \end{aligned}\right.</script><p>当然，这个函数可统一为</p><script type="math/tex; mode=display">L\left(f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)}), y^{(i)}\right)= - y^{(i)}\log \left(f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})\right) -(1-y^{(i)})\log \left(1-f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})\right)</script><p>其梯度下降公式为</p><script type="math/tex; mode=display">\begin{aligned} w_j &=w_j-\alpha\left[\frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right) x_j^{(i)}\right] \\ b &=b-\alpha\left[\frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)\right] \end{aligned}</script><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p><img src="/2022/11/30/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB/Snipaste_2022-12-02_16-05-37.png" alt></p><p><img src="/2022/11/30/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB/Snipaste_2022-12-02_16-11-39.png" alt></p><ul><li>欠拟合:</li></ul><p>欠拟合是指模型不能在<strong>训练集上获得足够低的误差</strong>。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p><ul><li>过拟合:</li></ul><p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong>。</p><p>解决过拟合可以使用以下三种方式:</p><ol><li>收集更多的数据</li><li>选择部分特征。而不是全部特征</li><li>减小参数大小。这可以通过正则化实现</li></ol><p>正则化(regularization)就是说给训练目标加上一些限制，比如说对于线性回归来说，正则化后的损失函数是</p><script type="math/tex; mode=display">J(\overrightarrow{\mathrm{w}}, b)=\frac{1}{2 m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2 m} \sum_{j=1}^n w_j^2</script><p>其中的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.027ex;" xmlns="http://www.w3.org/2000/svg" width="1.319ex" height="1.597ex" role="img" focusable="false" viewbox="0 -694 583 706"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"/></g></g></g></svg></mjx-container> 便起到限制作用，此时的梯度下降函数为</p><script type="math/tex; mode=display">w_n=w_n-\alpha \left[ \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right) x_n^{(i)}+\frac{\lambda}{m}w_n \right]</script><script type="math/tex; mode=display">b=b-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)</script><p>而正则化之后的逻辑回归损失函数为</p><script type="math/tex; mode=display">J({\overrightarrow{\mathrm{w}}, b})= -\frac{1}{m}\left[y^{(i)}\log \left(f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})\right) +(1-y^{(i)})\log \left(1-f_{\overrightarrow{\mathrm{w}}, b}(\small \overrightarrow{\mathrm{x}}^{(i)})\right)\right]+\frac{1}{2m}\sum_{j=1}^n w_j^2</script><p>而其梯度下降也与线性回归模型的函数一样。</p><script type="math/tex; mode=display">w_n=w_n-\alpha \left[ \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right) x_n^{(i)}+\frac{\lambda}{m}w_n \right]</script><script type="math/tex; mode=display">b=b-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)</script>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/11/29/ltest/"/>
    <url>/2022/11/29/ltest/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达-机器学习-多维特征</title>
    <link href="/2022/11/29/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81/"/>
    <url>/2022/11/29/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81/</url>
    
    <content type="html"><![CDATA[<p>我们常用向量来表达多维特征。在python中常用numpy来进行向量运算,对于</p><script type="math/tex; mode=display">f_{\overrightarrow{\mathrm{w}}, b}(\overrightarrow{\mathrm{x}})=\sum_{j=1}^n w_j x_j+b</script><p>即</p><script type="math/tex; mode=display">f_{\overrightarrow{\mathrm{w}}, b}(\overrightarrow{\mathrm{x}})=\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b</script><p>可以用numpy表达为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>w=np.array([<span class="hljs-number">1.0</span>,<span class="hljs-number">2.5</span>,-<span class="hljs-number">3.3</span>])<br>b=<span class="hljs-number">4</span><br>x=np.array([<span class="hljs-number">10</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>])<br>f=np.dot(w,x)+b<br></code></pre></td></tr></table></figure><p>向量运算的一大优点就是可以并行运算，运算速度比循环更快。</p><p>而多元线性回归模型的梯度下降公式为</p><script type="math/tex; mode=display">w_n=w_n-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right) x_n^{(i)}</script><script type="math/tex; mode=display">b=b-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)</script><h1 id="特征缩放-归一化"><a href="#特征缩放-归一化" class="headerlink" title="特征缩放(归一化)"></a>特征缩放(归一化)</h1><p>特征数量较多时，需要保证这些特征具有相近的尺度(无量纲化)，以保证梯度下降法更快的收敛，以下是四种常用的收敛方法。</p><h2 id="min-max标准化"><a href="#min-max标准化" class="headerlink" title="min-max标准化"></a>min-max标准化</h2><script type="math/tex; mode=display">\mathrm{x}^*=\frac{\mathrm{x}-\min (\mathrm{x})}{\max (\mathrm{x})-\min (\mathrm{x})}</script><h2 id="mean标准化"><a href="#mean标准化" class="headerlink" title="mean标准化"></a>mean标准化</h2><script type="math/tex; mode=display">\mathrm{x}^*=\frac{\mathrm{x}-\overline{\mathrm{x}}}{\max (\mathrm{x})-\min (\mathrm{x})}</script><h2 id="z-score标注化"><a href="#z-score标注化" class="headerlink" title="z-score标注化"></a>z-score标注化</h2><p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewbox="0 -431 571 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"/></g></g></g></svg></mjx-container>为标准差</p><script type="math/tex; mode=display">\mathrm{x}^*=\frac{\mathrm{x}-\overline{\mathrm{x}}}{\sigma}</script><h2 id="max标准化"><a href="#max标准化" class="headerlink" title="max标准化"></a>max标准化</h2><script type="math/tex; mode=display">\mathrm{x}^*=\frac{\mathrm{x}}{\max (\mathrm{x})}</script>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达-机器学习-线性回归模型(包括梯度下降)</title>
    <link href="/2022/11/25/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/11/25/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>对于线性模型</p><script type="math/tex; mode=display">f(x)=wx+b</script><p>来说w,b称为模型的参数(parameters,也可以称为coefficients,weights)</p><p>对于w,b来说我们的目标是如何让对于所有的训练集<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.138ex" height="2.587ex" role="img" focusable="false" viewbox="0 -893.3 4038.8 1143.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msup" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="TeXAtom" transform="translate(605,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g><g data-mml-node="mo" transform="translate(1838.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msup" transform="translate(2282.7,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="TeXAtom" transform="translate(523,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g><g data-mml-node="mo" transform="translate(3649.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>来说让<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.093ex" height="2.485ex" role="img" focusable="false" viewbox="0 -893.3 1367.1 1098.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(523,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></g></g></svg></mjx-container>与<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.093ex" height="2.485ex" role="img" focusable="false" viewbox="0 -893.3 1367.1 1098.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="TeXAtom" transform="translate(523,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></g></g></svg></mjx-container>最小</p><p><img src="/2022/11/25/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/Snipaste_2022-11-25_21-35-47.png" alt></p><p>所以它的代价函数(cost function)也十分简单:</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{2m}\sum\limits_{i=1}^m{(\hat y^{(i)}-\hat y ^{(i)})^2}</script><p>这里的2是未来方便运算，分母的m是确保随着训练集样本数量的变大，代价函数的值不会随之增大</p><h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>在获得损失函数J(w,b)之后，我们希望当J(w,b)取最小值时的的参数w,b的值，可以通过梯度下降来实现</p><script type="math/tex; mode=display">w=w-\alpha \frac{\partial}{\partial w} J(w, b)</script><script type="math/tex; mode=display">b=b-\alpha \frac{\partial}{\partial b} J(w, b)</script><p>其中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"/></g></g></g></svg></mjx-container> 是学习率<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex;" xmlns="http://www.w3.org/2000/svg" width="9.835ex" height="2.852ex" role="img" focusable="false" viewbox="0 -899.6 4347.2 1260.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(473.1,394) scale(0.707)"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g></g><rect width="1106.5" height="60" x="120" y="220"/></g><g data-mml-node="mi" transform="translate(1346.5,0)"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"/></g><g data-mml-node="mo" transform="translate(1979.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(2368.5,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mo" transform="translate(3084.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(3529.2,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mo" transform="translate(3958.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 是J(w,b)对w的偏导，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex;" xmlns="http://www.w3.org/2000/svg" width="9.376ex" height="2.852ex" role="img" focusable="false" viewbox="0 -899.6 4144.2 1260.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(371.7,394) scale(0.707)"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g></g><rect width="903.6" height="60" x="120" y="220"/></g><g data-mml-node="mi" transform="translate(1143.6,0)"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"/></g><g data-mml-node="mo" transform="translate(1776.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(2165.6,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mo" transform="translate(2881.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(3326.2,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mo" transform="translate(3755.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 是J(w,b)对b的偏导。</p><p>显然,<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"/></g></g></g></svg></mjx-container> 如果很小，那么梯度下降可能会非常慢(收敛很慢)。如果<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"/></g></g></g></svg></mjx-container> 非常大,可能导致过冲(overshoot)，震荡，甚至永远不会到达最小值，甚至可能发散。</p><p><img src="/2022/11/25/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/Snipaste_2022-11-29_16-23-09.png" alt></p><p>但是固定的学习率<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"/></g></g></g></svg></mjx-container> 能让损失函数到达局部最小值，因为随着到达局部最小值，导数的值会变小趋近于0，这意味这更新步骤也会越边越小。</p><p><img src="/2022/11/25/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/Snipaste_2022-11-29_16-35-12.png" alt></p><p>对于线性回归的模型来说，梯度下降为</p><script type="math/tex; mode=display">w=w-\alpha \frac{1}{m} \sum_{i=1}\left(f_{w, b}(x^{(i)})-y^{(i)}\right) x^{(i)}</script><script type="math/tex; mode=display">b=b-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{w, b}(x^{(i)})-y^{(i)}\right)</script><p>同时线性模型的损失函数是一个凸函数(convex function)，除了全局最小值，它没有任何局部最小值</p><p><img src="/2022/11/25/%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/Snipaste_2022-11-29_16-55-20.png" alt></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>监督学习</title>
    <link href="/2022/11/24/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/11/24/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># %%</span><br><span class="hljs-keyword">import</span> sklearn <span class="hljs-keyword">as</span> sk<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_olivetti_faces<br>faces=fetch_olivetti_faces()<br><span class="hljs-comment"># print(faces.DESCR)</span><br><br><span class="hljs-comment"># %%</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_faces</span>(<span class="hljs-params">images,target,top_n</span>):<br>    fig =plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">12</span>))<span class="hljs-comment">#创造一个画板</span><br>    fig.subplots_adjust(left=<span class="hljs-number">0</span>,right=<span class="hljs-number">1</span>,bottom=<span class="hljs-number">0</span>,top=<span class="hljs-number">1</span>,hspace=<span class="hljs-number">0.05</span>,wspace=<span class="hljs-number">0.05</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top_n):<br>        p=fig.add_subplot(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,i+<span class="hljs-number">1</span>,xticks=[],yticks=[])<br>        p.imshow(images[i],cmap=plt.cm.bone)<br>        p.text(<span class="hljs-number">0</span>,<span class="hljs-number">14</span>,<span class="hljs-built_in">str</span>(target[i]))<br>        p.text(<span class="hljs-number">0</span>,<span class="hljs-number">60</span>,<span class="hljs-built_in">str</span>(i))<br>print_faces(faces.images,faces.target,<span class="hljs-number">20</span>)<br><br><span class="hljs-comment"># %%</span><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_evaluate</span>(<span class="hljs-params">clf,X_train,X_test,Y_train,Y_test</span>):<br>    clf.fit(X_train,Y_train)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Accuracy on training set:"</span>)<br>    <span class="hljs-built_in">print</span>(clf.score(X_train,Y_train))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Accuracy on testing set"</span>)<br>    <span class="hljs-built_in">print</span>(clf.score(X_test,Y_test))<br><br>    Y_pred=clf.predict(X_test)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Classification Report:"</span>)<br>    <span class="hljs-built_in">print</span>(metrics.classification_report(Y_test,Y_pred))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Confusion Matrix"</span>)<br>    <span class="hljs-built_in">print</span>(metrics.confusion_matrix(Y_test,Y_pred))<br><br><span class="hljs-comment"># %%</span><br><span class="hljs-comment">#定义函数来评估K交叉验证</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score,KFold<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> sem<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_cross_validation</span>(<span class="hljs-params">clf,X,Y,K</span>):<br>    cv=KFold(K,shuffle=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>)<br>    scores=cross_val_score(clf,X,Y,cv=cv)<br>    <span class="hljs-built_in">print</span>(scores)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Mean score: {0:.3f} (+/-{1:.3f})"</span>.<span class="hljs-built_in">format</span>(np.mean(scores),sem(scores)))<br><br><br><span class="hljs-comment"># %%</span><br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC <br><span class="hljs-keyword">from</span>   sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br><span class="hljs-comment"># the index ranges of images of people with glasses</span><br>glasses = [<br>   (<span class="hljs-number">10</span>, <span class="hljs-number">19</span>), (<span class="hljs-number">30</span>, <span class="hljs-number">32</span>), (<span class="hljs-number">37</span>, <span class="hljs-number">38</span>), (<span class="hljs-number">50</span>, <span class="hljs-number">59</span>), (<span class="hljs-number">63</span>, <span class="hljs-number">64</span>),<br>   (<span class="hljs-number">69</span>, <span class="hljs-number">69</span>), (<span class="hljs-number">120</span>, <span class="hljs-number">121</span>), (<span class="hljs-number">124</span>, <span class="hljs-number">129</span>), (<span class="hljs-number">130</span>, <span class="hljs-number">139</span>), (<span class="hljs-number">160</span>, <span class="hljs-number">161</span>),<br>   (<span class="hljs-number">164</span>, <span class="hljs-number">169</span>), (<span class="hljs-number">180</span>, <span class="hljs-number">182</span>), (<span class="hljs-number">185</span>, <span class="hljs-number">185</span>), (<span class="hljs-number">189</span>, <span class="hljs-number">189</span>), (<span class="hljs-number">190</span>, <span class="hljs-number">192</span>),<br>   (<span class="hljs-number">194</span>, <span class="hljs-number">194</span>), (<span class="hljs-number">196</span>, <span class="hljs-number">199</span>), (<span class="hljs-number">260</span>, <span class="hljs-number">269</span>), (<span class="hljs-number">270</span>, <span class="hljs-number">279</span>), (<span class="hljs-number">300</span>, <span class="hljs-number">309</span>),<br>   (<span class="hljs-number">330</span>, <span class="hljs-number">339</span>), (<span class="hljs-number">358</span>, <span class="hljs-number">359</span>), (<span class="hljs-number">360</span>, <span class="hljs-number">369</span>)<br>]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_targe</span>(<span class="hljs-params">segments</span>):<br>    y=np.zeros(faces.target.shape[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">for</span> (start,end) <span class="hljs-keyword">in</span> segments:<br>        y[start:end+<span class="hljs-number">1</span>]=<span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> y<br>target_glasses=create_targe(glasses)<br>X_train,X_test,Y_train,Y_test=train_test_split(faces.data,target_glasses,test_size=<span class="hljs-number">0.25</span>,random_state=<span class="hljs-number">0</span>)<br>svc_2=SVC(kernel=<span class="hljs-string">'linear'</span>)<br><br><br><span class="hljs-comment"># %%</span><br>evaluate_cross_validation(svc_2,X_train,Y_train,<span class="hljs-number">5</span>)<br>train_and_evaluate(svc_2,X_train,X_test,Y_train,Y_test)<br><br><span class="hljs-comment"># %%</span><br>X_test = faces.data[<span class="hljs-number">30</span>:<span class="hljs-number">40</span>]<br>Y_test= target_glasses[<span class="hljs-number">30</span>:<span class="hljs-number">40</span>]<br><span class="hljs-built_in">print</span>(Y_test.shape[<span class="hljs-number">0</span>])<br>select = np.ones(target_glasses.shape[<span class="hljs-number">0</span>])<br>X_train = faces.data[select ==  <span class="hljs-number">1</span>]<br>Y_train = target_glasses[select ==  <span class="hljs-number">1</span>]<br>svc_3=SVC(kernel=<span class="hljs-string">'linear'</span>)<br>train_and_evaluate(svc_3,X_train,X_test,Y_train,Y_test)<br><br><br><br><span class="hljs-comment"># %%</span><br>Y_pred=svc_3.predict(X_test)<br>eval_faces=[np.reshape(a,(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>)) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> X_test]<br>print_faces(eval_faces,Y_pred,<span class="hljs-number">10</span>)<br><br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-温和的介绍</title>
    <link href="/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B8%A9%E5%92%8C%E7%9A%84%E4%BB%8B%E7%BB%8D/"/>
    <url>/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B8%A9%E5%92%8C%E7%9A%84%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习概念"><a href="#机器学习概念" class="headerlink" title="机器学习概念"></a>机器学习概念</h1><p>任何机器学习都可以通过以下三个概念表示:</p><p>任务T：比如构建一个垃圾邮件过滤器，将电子邮件归类为垃圾邮件或正常邮件。</p><p>经验E：通过经验来学习执行任务。比如对于垃圾邮件过滤器，需要将一组电子邮件人工分类为电子邮件或正常邮件。</p><p>表现P:来体现表现任务的能力，比如我们的垃圾邮件过滤器将电子邮件正确分类为垃圾邮件或正常邮件的百分比。</p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>机器学习方法依赖于以前的经验，通常由数据集表示。这里使用鸢尾花数据集，它包含来自三种不同鸢尾花物种的150个实例，包括萼片和花瓣的长度和宽度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><br>iris = datasets.load_iris()<br>X_iris, Y_iris = iris.data, iris.target<br><span class="hljs-built_in">print</span>(X_iris.shape, Y_iris.shape)<span class="hljs-comment"># (150, 4) (150,)</span><br><span class="hljs-built_in">print</span>(X_iris[<span class="hljs-number">0</span>], Y_iris[<span class="hljs-number">0</span>]) <span class="hljs-comment"># [5.1 3.5 1.4 0.2] 0</span><br></code></pre></td></tr></table></figure><p><strong>iris</strong>数据集是一个对象，它有两个主要部分:</p><p>一个<strong>data</strong>数组，对于每个实例，都有萼片长度，萼片宽度，花瓣长度，花瓣宽度。（scikit-learn使用ndarrays,而不是更具描述性但效率更低的Python词典或列表。这个数组的shape是(150,4)，代表有150行实例和每个实例4个特征</p><p>一个<strong>targe</strong>数组，值在0到2的范围，分别对应(0:山鸢尾，1:杂色鸢尾，2:弗吉尼亚鸢尾)</p><h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><p>线性分类模型试图做的:构建一条线(或者说特征空间的超平面)，最优的分离两个目标类，并将其用作决策边界</p><p>这里引入一个简单的学习任务：我们瞄准鸢尾花实例，以预测它是否是一个山鸢尾。这是一个二元分类任务。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>iris = datasets.load_iris()<br>X_iris, Y_iris = iris.data, iris.target<br>X, Y = X_iris[:, :<span class="hljs-number">2</span>], Y_iris<br>X_train, X_test, Y_train, Y_test = train_test_split(<br>    X, Y, test_size=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">33</span>)<br><br>scaler = preprocessing.StandardScaler().fit(X_train)<br>X_train = scaler.transform(X_train)<br>X_test = scaler.transform(X_test)<br><br>colors = [<span class="hljs-string">'red'</span>, <span class="hljs-string">'green'</span>, <span class="hljs-string">'blue'</span>]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(colors)):<br>    xs = X_train[:, <span class="hljs-number">0</span>][Y_train == i]<br>    ys = X_train[:, <span class="hljs-number">1</span>][Y_train == i]<br>    plt.scatter(xs, ys, c=colors[i])<br><br>plt.legend(iris.target_names)<br>plt.xlabel(<span class="hljs-string">'Sepal length'</span>)<br>plt.ylabel(<span class="hljs-string">'Sepal width'</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p><img src="/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B8%A9%E5%92%8C%E7%9A%84%E4%BB%8B%E7%BB%8D/Figure_1.png" alt="Figure_1"></p><p>实现线性分类，需要使用scikit-learn中的SGDClassfuer,SGD代表随机梯度下降，用于查找函数的局部最小值（本例中为损失函数），fit函数接受训练数据和训练类别，并且构造分类器。现在根据clf的属性有</p><script type="math/tex; mode=display">clf.intercept_[i]+x1*clf.coef_[i,0]+clf.coef[i,1]*x2=0</script><p>即给定x1和x2的值，如果它的值大于0，它就在决策边界之上。但是系数矩阵有三行，代表它提出了三个将一个类与其他类别分开的直线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> s了klearn <span class="hljs-keyword">import</span> preprocessing<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> SGDClassifier<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>iris = datasets.load_iris()<br>X_iris, Y_iris = iris.data, iris.target<br>X, Y = X_iris[:, :<span class="hljs-number">2</span>], Y_iris<br>X_train, X_test, Y_train, Y_test = train_test_split(<br>    X, Y, test_size=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">33</span>)<br><br>scaler = preprocessing.StandardScaler().fit(X_train)<br>X_train = scaler.transform(X_train)<br>X_test = scaler.transform(X_test)<br><br><span class="hljs-comment"># colors = ['red', 'green', 'blue']</span><br><span class="hljs-comment"># for i in range(len(colors)):</span><br><span class="hljs-comment">#     xs = X_train[:, 0][Y_train == i]</span><br><span class="hljs-comment">#     ys = X_train[:, 1][Y_train == i]</span><br><span class="hljs-comment">#     plt.scatter(xs, ys, c=colors[i])</span><br><br><span class="hljs-comment"># plt.legend(iris.target_names)</span><br><span class="hljs-comment"># plt.xlabel('Sepal length')</span><br><span class="hljs-comment"># plt.ylabel('Sepal width')</span><br><span class="hljs-comment"># plt.show()</span><br><br>clf=SGDClassifier()<br>clf.fit(X_train,Y_train)<br>Y_train_pred=clf.predict(X_train)<br><span class="hljs-comment"># print(metrics.accuracy_score(Y_train,Y_train_pred))</span><br><span class="hljs-comment"># print(clf.coef_)</span><br><br>x_min,x_max=X_train[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>()-<span class="hljs-number">.5</span>,X_train[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">.5</span><br>y_min,y_max=X_train[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>()-<span class="hljs-number">.5</span>,X_train[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">.5</span><br>xs=np.arange(x_min,x_max,<span class="hljs-number">0.5</span>)<br>fig,axes=plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)<br>fig.set_size_inches(<span class="hljs-number">10</span>,<span class="hljs-number">6</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>    axes[i].set_aspect(<span class="hljs-string">'equal'</span>)<br>    axes[i].set_title(<span class="hljs-built_in">str</span>(i))<br>    axes[i].set_xlabel(<span class="hljs-string">'length'</span>)<br>    axes[i].set_ylabel(<span class="hljs-string">'width'</span>)<br>    axes[i].set_xlim(x_min,x_max)<br>    axes[i].set_ylim(y_min,y_max)<br>    plt.sca(axes[i])<br>    plt.scatter(X_train[:,<span class="hljs-number">0</span>],X_train[:,<span class="hljs-number">1</span>],c=Y_train,cmap=plt.cm.prism)<br>    ys=(-clf.intercept_[i]-xs*clf.coef_[i,<span class="hljs-number">0</span>])/clf.coef_[i,<span class="hljs-number">1</span>]<span class="hljs-comment">#clf.intercept_[i]+xs*clf.coef_[i,0]+clf.coef[i,1]*ys=0</span><br>    plt.plot(xs,ys)<br>    <br><br>plt.show()<br><br></code></pre></td></tr></table></figure><p><img src="/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B8%A9%E5%92%8C%E7%9A%84%E4%BB%8B%E7%BB%8D/res.png" alt="res"></p><h1 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h1><div class="table-container"><table><thead><tr><th></th><th>预测：正例</th><th>预测:负例</th></tr></thead><tbody><tr><td>目标:正例</td><td>真正例(TP)</td><td>假负例(FN)</td></tr><tr><td>目标:负例</td><td>假正例(FP)</td><td>真负例(TN)</td></tr></tbody></table></div><p>m为样本量即m=TP+TN+FP+FB</p><p>准确率=(TP+TN)/m</p><p>精确率= TP /（TP+FP）</p><p>召回率=TP /   (TP+FN)</p><p>F1 得分 =2  <em> 精确率  </em> 召回 / （精确率 + 召回率)</p><p>sklearn中可以如下所用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> SGDClassifier<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>iris = datasets.load_iris()<br>X_iris, Y_iris = iris.data, iris.target<br>X, Y = X_iris[:, :<span class="hljs-number">2</span>], Y_iris<br>X_train, X_test, Y_train, Y_test = train_test_split(<br>    X, Y, test_size=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">33</span>)<br><br>scaler = preprocessing.StandardScaler().fit(X_train)<br>X_train = scaler.transform(X_train)<br>X_test = scaler.transform(X_test)<br><br><br><br>clf=SGDClassifier()<br>clf.fit(X_train,Y_train)<br>Y_pred=clf.predict(X_test)<br><span class="hljs-built_in">print</span>(metrics.classification_report(Y_test,Y_pred,target_names=iris.target_names))<br><br></code></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs ini">precision    recall  f1-score   support<br><br>      setosa       1.00      1.00      1.00         8<br>  versicolor       0.00      0.00      0.00        11<br>   virginica       0.63      1.00      0.78        19<br><br>    accuracy                           0.71        38<br>   macro avg       0.54      0.67      0.59        38<br>weighted avg       0.53      0.71      0.60        38<br></code></pre></td></tr></table></figure><p>此外，对于评估过程，我们可以使用交叉验证。K折交叉验证的常用步骤如下:</p><ol><li>将数据集划分为K个不同子集</li><li>通过训练k-1个子集并测试剩余的一个子集来创建k个不同模型</li><li>测量k个模型的表现并取平均值</li></ol><p>比如k=5就代表每次训练80%数据并测试剩余20%</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">*<span class="hljs-keyword">from</span>* sklearn.model_selection *<span class="hljs-keyword">import</span>* cross_val_score, KFold<br><br>*<span class="hljs-keyword">from</span>* sklearn.pipeline *<span class="hljs-keyword">import</span>* Pipeline<br><br>*<span class="hljs-keyword">from</span>* sklearn.linear_model *<span class="hljs-keyword">import</span>* SGDClassifier<br><br>*<span class="hljs-keyword">from</span>* sklearn *<span class="hljs-keyword">import</span>* datasets<br><br>*<span class="hljs-keyword">from</span>* sklearn *<span class="hljs-keyword">import</span>* preprocessing<br><br><br><br>iris = datasets.load_iris()<br><br>X_iris, Y_iris = iris.data, iris.target<br><br>X, Y = X_iris[:, :<span class="hljs-number">2</span>], Y_iris<br><br>clf = Pipeline([<br><br>  (<span class="hljs-string">'scaler'</span>, preprocessing.StandardScaler()),<br><br>  (<span class="hljs-string">'linear_model'</span>, SGDClassifier())<br><br>])<br><br><br><br>cv = KFold(<span class="hljs-number">5</span>, *shuffle*=<span class="hljs-literal">True</span>, *random_state*=<span class="hljs-number">22</span>)<br><br>score = cross_val_score(clf, X, Y, *cv*=cv)<br><br><span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># [0.76666667 0.76666667 0.86666667 0.73333333 0.7       ]</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>sylar服务器框架</title>
    <link href="/2022/11/05/sylar%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A1%86%E6%9E%B6/"/>
    <url>/2022/11/05/sylar%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="sylar"><a href="#sylar" class="headerlink" title="sylar"></a>sylar</h1><h2 id="日志系统"><a href="#日志系统" class="headerlink" title="日志系统"></a>日志系统</h2><p>1)<br>    Log4j</p><pre><code class="hljs">Logger(定义日志类别)  |  |---------Formatter(日志格式)  |Appender(日志输出地方)</code></pre><h2 id="协程库封装"><a href="#协程库封装" class="headerlink" title="协程库封装"></a>协程库封装</h2><h2 id="socket函数库"><a href="#socket函数库" class="headerlink" title="socket函数库"></a>socket函数库</h2><h2 id="http协议开发"><a href="#http协议开发" class="headerlink" title="http协议开发"></a>http协议开发</h2><h2 id="分布协议"><a href="#分布协议" class="headerlink" title="分布协议"></a>分布协议</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>测试</title>
    <link href="/2022/11/05/%E6%B5%8B%E8%AF%95/"/>
    <url>/2022/11/05/%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<p>测试一下</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
